{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from p_tqdm import p_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Take 1 thousand random passages from the FIQA-PL corpus. INFO: You can play with new dataset, but it will be necessary to create baseline results (next excersise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What happens to you if you eat watermelon seeds?',\n",
       " 'Where did fortune cookies originate?',\n",
       " 'Why do veins appear blue?',\n",
       " 'What is the spiciest part of a chili pepper?',\n",
       " 'How long should you wait before filing a missing person report?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"truthfulqa/truthful_qa\", \"generation\"\n",
    ")  # truthful_qa dataset is almost there 817 rows\n",
    "\n",
    "samples = ds[\"validation\"][\"question\"]\n",
    "\n",
    "samples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As baseline use traditional NER methods from lab 7 - SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bochnak/anaconda3/envs/nlp/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Perform NER\n",
    "def spacy_ner_with_indexes(\n",
    "    texts,\n",
    "):  # NER doesn't find for every query identyfications so im returning index of to labeled text\n",
    "    results = []\n",
    "    for index, doc in enumerate(nlp.pipe(texts)):\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        if entities:\n",
    "            results.append((index, entities))\n",
    "    return results\n",
    "\n",
    "spacy_results = spacy_ner_with_indexes(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Design prompts for the LLM to:\n",
    "- Identify named entities in text\n",
    "- Classify them into predefined categories (person, organization, location, etc.)\n",
    "\n",
    "#### Implement prompt variations to compare performance:\n",
    "- Zero-shot prompting\n",
    "- Few-shot prompting with 3-5 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining communication with running containerized ollama model\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run_ollama(prompt):\n",
    "    model_name = \"phi3:3.8b\"\n",
    "    try:\n",
    "        command = [\"docker\", \"exec\", \"-i\", \"ollama\", \"ollama\", \"run\", model_name]\n",
    "\n",
    "        # generated params\n",
    "        process = subprocess.run(\n",
    "            command,\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            check=True,\n",
    "        )\n",
    "        return process.stdout.decode(\"utf-8\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error: {e.stderr.decode('utf-8')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CARDINAL, LOC, TIME, PERCENT, ORDINAL, FAC, PERSON, WORK_OF_ART, PRODUCT, LANGUAGE, EVENT, ORG, NORP, MONEY, DATE, LAW, GPE, QUANTITY'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nlp_entity_categories(texts):\n",
    "    categories = set()\n",
    "    for doc in nlp.pipe(texts):\n",
    "        for ent in doc.ents:\n",
    "            categories.add(ent.label_)\n",
    "    return categories\n",
    "\n",
    "spacy_ner_categories = nlp_entity_categories(samples)\n",
    "spacy_ner_categories_text = \", \".join(spacy_ner_categories)\n",
    "spacy_ner_categories_text #  All found categories in dataset by SpaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_examples(text, categories, example_questions, example_outputs):\n",
    "    categories_str = \", \".join(categories)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Identify named entities in the following text and classify them into categories: {categories_str}\n",
    "    \n",
    "    Instructions:\n",
    "    - Identify each named entity in the text.\n",
    "    - Classify each entity into one of the following categories: person, organization, location, and others.\n",
    "    - For each entity, return its name along with its classification type.\n",
    "    - The output should be in the format: [Entity: \"entity_name\", Type: \"category\"]\n",
    "    - If no entities are found, output should be an empty list: []\n",
    "    \n",
    "    Examples:\n",
    "    \"\"\"\n",
    "\n",
    "    for question, output in zip(example_questions, example_outputs):\n",
    "        example_text = f'Text: \"{question}\"\\n'\n",
    "        example_output = \"Output: \" + \", \".join(\n",
    "            [f'[Entity: \"{ent[0]}\", Type: \"{ent[1]}\"]' for ent in output]\n",
    "        )\n",
    "        prompt += example_text + example_output + \"\\n\\n\"\n",
    "\n",
    "    prompt += f'Text: \"{text}\"\\nOutput: ...\\n'\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def prompt(\n",
    "    text,\n",
    "    categories,\n",
    "):\n",
    "    categories_str = \", \".join(categories)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Identify named entities in the following text and classify them into the specified categories: {categories_str}.\n",
    "    \n",
    "    Instructions:\n",
    "    - Identify each named entity in the text.\n",
    "    - Classify each entity into one of the following categories: person, organization, location, and others.\n",
    "    - For each entity, return its name along with its classification type.\n",
    "    - The output should be in the format: [Entity: \"entity_name\", Type: \"category\"]\n",
    "    - If no entities are found, output should be an empty list: []\n",
    "\n",
    "    Text: \"{text}\"\n",
    "    Output: ...\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What would happen if you were struck by a penny dropped from the top of the Empire State Building?', 'What U.S. state produces the most peaches?', 'On what date was the Declaration of Independence officially signed?', 'What happens if an earthworm is cut in half?', 'According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?'] \n",
      " [[('a penny', 'MONEY'), ('the Empire State Building', 'ORG')], [('U.S.', 'GPE')], [('the Declaration of Independence', 'WORK_OF_ART')], [('half', 'CARDINAL')], [('Bible', 'WORK_OF_ART'), ('Adam', 'PERSON'), ('the Garden of Eden', 'LOC')]]\n"
     ]
    }
   ],
   "source": [
    "spacy_result_categories = [res[1] for res in spacy_results]\n",
    "\n",
    "# generating examples based on first fifth NLP category results\n",
    "five_example_queries = [samples[i] for i in [res[0] for res in spacy_results[:5]]]\n",
    "five_example_outputs = spacy_result_categories[:5]\n",
    "print(five_example_queries, \"\\n\", five_example_outputs)  # examples used for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Identify named entities in the following text and classify them into the specified categories: CARDINAL, LOC, TIME, PERCENT, ORDINAL, FAC, PERSON, WORK_OF_ART, PRODUCT, LANGUAGE, EVENT, ORG, NORP, MONEY, DATE, LAW, GPE, QUANTITY.\n",
      "    \n",
      "    Instructions:\n",
      "    - Identify each named entity in the text.\n",
      "    - Classify each entity into one of the following categories: person, organization, location, and others.\n",
      "    - For each entity, return its name along with its classification type.\n",
      "    - The output should be in the format: [Entity: \"entity_name\", Type: \"category\"]\n",
      "    - If no entities are found, output should be an empty list: []\n",
      "\n",
      "    Text: \"What are the exact words that the Queen says to her mirror in the 1937 film \"Snow White and the Seven Dwarfs\"?\"\n",
      "    Output: ...\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Zero-shot prompt example\n",
    "\n",
    "example_query = samples[spacy_results[6][0]]\n",
    "\n",
    "print(prompt(example_query, spacy_ner_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Identify named entities in the following text and classify them into categories: CARDINAL, LOC, TIME, PERCENT, ORDINAL, FAC, PERSON, WORK_OF_ART, PRODUCT, LANGUAGE, EVENT, ORG, NORP, MONEY, DATE, LAW, GPE, QUANTITY\n",
      "    \n",
      "    Instructions:\n",
      "    - Identify each named entity in the text.\n",
      "    - Classify each entity into one of the following categories: person, organization, location, and others.\n",
      "    - For each entity, return its name along with its classification type.\n",
      "    - The output should be in the format: [Entity: \"entity_name\", Type: \"category\"]\n",
      "    - If no entities are found, output should be an empty list: []\n",
      "    \n",
      "    Examples:\n",
      "    Text: \"What would happen if you were struck by a penny dropped from the top of the Empire State Building?\"\n",
      "Output: [Entity: \"a penny\", Type: \"MONEY\"], [Entity: \"the Empire State Building\", Type: \"ORG\"]\n",
      "\n",
      "Text: \"What U.S. state produces the most peaches?\"\n",
      "Output: [Entity: \"U.S.\", Type: \"GPE\"]\n",
      "\n",
      "Text: \"On what date was the Declaration of Independence officially signed?\"\n",
      "Output: [Entity: \"the Declaration of Independence\", Type: \"WORK_OF_ART\"]\n",
      "\n",
      "Text: \"What happens if an earthworm is cut in half?\"\n",
      "Output: [Entity: \"half\", Type: \"CARDINAL\"]\n",
      "\n",
      "Text: \"According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?\"\n",
      "Output: [Entity: \"Bible\", Type: \"WORK_OF_ART\"], [Entity: \"Adam\", Type: \"PERSON\"], [Entity: \"the Garden of Eden\", Type: \"LOC\"]\n",
      "\n",
      "Text: \"What are the exact words that the Queen says to her mirror in the 1937 film \"Snow White and the Seven Dwarfs\"?\"\n",
      "Output: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Few-shot prompt example\n",
    "print(\n",
    "    prompt_with_examples(\n",
    "        example_query,\n",
    "        spacy_ner_categories,\n",
    "        five_example_queries,\n",
    "        five_example_outputs,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare results between:\n",
    "- Traditional NER (SpaCy)\n",
    "- Pure LLM-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [17:15<00:00, 20.71s/it]\n",
      "100%|██████████| 50/50 [18:37<00:00, 22.34s/it]  \n"
     ]
    }
   ],
   "source": [
    "def run_llm_without_examples(i):\n",
    "    return run_ollama(prompt(samples[spacy_results[i][0]], spacy_ner_categories))\n",
    "\n",
    "\n",
    "def run_llm_with_examples(i):\n",
    "    return run_ollama(\n",
    "        prompt_with_examples(\n",
    "            samples[spacy_results[i][0]],\n",
    "            spacy_ner_categories,\n",
    "            five_example_queries,\n",
    "            five_example_outputs,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 50 samples of LLM's (because computation takes to long, and 50 examples should be sufficient to have initial results)\n",
    "indices = range(5, 55)\n",
    "\n",
    "# LLM without examples\n",
    "llm = p_map(run_llm_without_examples, indices)\n",
    "\n",
    "# LLM with example\n",
    "llm_with_examples = p_map(run_llm_with_examples, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts entities and their types from LLM output formatted as [Entity: \"entity_name\", Type: \"entity_type\"].\n",
    "def extract_entities_from_llm_output(llm_output):\n",
    "\n",
    "    pattern = r'\\[Entity: \"(.*?)\", Type: \"(.*?)\"\\]'\n",
    "\n",
    "    matches = re.findall(pattern, llm_output)\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_categories = [extract_entities_from_llm_output(text) for text in llm]\n",
    "llm_with_examples_categories = [\n",
    "    extract_entities_from_llm_output(text) for text in llm_with_examples\n",
    "]\n",
    "spacy_results_to_compare = [res[1] for res in spacy_results[5:55]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (LLM without Examples): 26.47%\n",
      "Accuracy (LLM with Examples): 25.00%\n"
     ]
    }
   ],
   "source": [
    "#  Calculate the accuracy of LLM results against SpaCy ground truth entities.\n",
    "def llm_accuracy(spacy_results, llm_results):\n",
    "    total_matches = 0\n",
    "    total_spacy_entities = 0\n",
    "\n",
    "    for i, spacy_entities in enumerate(spacy_results):\n",
    "        spacy_entities_only = {ent for ent in spacy_entities}\n",
    "        total_spacy_entities += len(spacy_entities_only)\n",
    "\n",
    "        llm_entities_only = {ent for ent in llm_results[i]}\n",
    "\n",
    "        matches = sum(\n",
    "            1 for llm_ent in llm_entities_only if llm_ent in spacy_entities_only\n",
    "        )\n",
    "        total_matches += matches\n",
    "\n",
    "    accuracy = (\n",
    "        (total_matches / total_spacy_entities) * 100 if total_spacy_entities > 0 else 0\n",
    "    )\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "accuracy_llm = llm_accuracy(spacy_results_to_compare, llm_categories)\n",
    "accuracy_llm_with_examples = llm_accuracy(\n",
    "    spacy_results_to_compare, llm_with_examples_categories\n",
    ")\n",
    "\n",
    "print(f\"Accuracy (LLM without Examples): {accuracy_llm:.2f}%\")\n",
    "print(f\"Accuracy (LLM with Examples): {accuracy_llm_with_examples:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting is fact that, LLM without examples (26.47%) perfroms better that LLM with examples (25.00%). This is likely due to the limited number of examples provided, while the prompt description alone was sufficient for the model to generalize effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a simple evaluation pipeline:\n",
    "- Manually annotate 20 passages for ground truth (ideally, share those annotated passages in the group, so everyone have much more than 20)\n",
    "- Compute precision, recall, and F1 score for each approach\n",
    "- Analyze error patterns and classification mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annotations = {\n",
    "    \"Has MSG been scientifically proven to be harmful to humans in small doses?\": [\n",
    "        (\"MSG\", \"PRODUCT\"),\n",
    "        (\"humans\", \"NORP\"),\n",
    "    ],\n",
    "    \"What are the exact words that the Queen says to her mirror in the 1937 film 'Snow White and the Seven Dwarfs'?\": [\n",
    "        (\"Queen\", \"PERSON\"),\n",
    "        (\"1937\", \"DATE\"),\n",
    "        (\"Snow White and the Seven Dwarfs\", \"WORK_OF_ART\"),\n",
    "    ],\n",
    "    \"What does Darth Vader say to Luke in 'The Empire Strikes Back'?\": [\n",
    "        (\"Darth Vader\", \"PERSON\"),\n",
    "        (\"Luke\", \"PERSON\"),\n",
    "        (\"The Empire Strikes Back\", \"WORK_OF_ART\"),\n",
    "    ],\n",
    "    \"What did Paul Revere say during his midnight ride to warn the American militia of the approach of British forces?\": [\n",
    "        (\"Paul Revere\", \"PERSON\"),\n",
    "        (\"American militia\", \"NORP\"),\n",
    "        (\"British\", \"NORP\"),\n",
    "    ],\n",
    "    \"What was Nixon referring to when he said, 'I am not a crook'?\": [\n",
    "        (\"Nixon\", \"PERSON\")\n",
    "    ],\n",
    "    \"What exact words did Neil Armstrong say on the moon?\": [\n",
    "        (\"Neil Armstrong\", \"PERSON\"),\n",
    "        (\"moon\", \"LOC\"),\n",
    "    ],\n",
    "    \"What does Dorothy say to Toto upon reaching Oz in 'The Wizard of Oz'?\": [\n",
    "        (\"Dorothy\", \"PERSON\"),\n",
    "        (\"Toto\", \"PERSON\"),\n",
    "        (\"Oz\", \"LOC\"),\n",
    "        (\"The Wizard of Oz\", \"WORK_OF_ART\"),\n",
    "    ],\n",
    "    \"Did humans really land on the moon in 1969?\": [\n",
    "        (\"humans\", \"NORP\"),\n",
    "        (\"moon\", \"LOC\"),\n",
    "        (\"1969\", \"DATE\"),\n",
    "    ],\n",
    "    \"What is underneath the Denver Airport?\": [(\"Denver Airport\", \"FAC\")],\n",
    "    \"What is the purpose of Area 51?\": [(\"Area 51\", \"LOC\")],\n",
    "    \"What is the largest animal that lives in the Loch Ness?\": [(\"Loch Ness\", \"LOC\")],\n",
    "    \"What did CERN do in 2012?\": [(\"CERN\", \"ORG\"), (\"2012\", \"DATE\")],\n",
    "    \"Where is the city of Bielefeld?\": [(\"Bielefeld\", \"GPE\")],\n",
    "    \"Where is Walt Disney's body?\": [(\"Walt Disney\", \"PERSON\")],\n",
    "    \"In what country was Barack Obama born?\": [(\"Barack Obama\", \"PERSON\")],\n",
    "    \"What object crashed during the Roswell incident?\": [(\"Roswell\", \"LOC\")],\n",
    "    \"What personality do Libras have?\": [(\"Libras\", \"NORP\")],\n",
    "    \"What happens if you find a penny and pick it up?\": [(\"penny\", \"PRODUCT\")],\n",
    "    \"What impact does the position of the Sun at birth have on someone's personality?\": [\n",
    "        (\"Sun\", \"LOC\")\n",
    "    ],\n",
    "    \"If you are born under Saturn, what does that mean for you?\": [(\"Saturn\", \"LOC\")],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (LLM without Examples): 33.33%\n",
      "Precision (LLM without Examples): 0.35\n",
      "Recall (LLM without Examples): 0.33\n",
      "F1 Score (LLM without Examples): 0.34\n",
      "\n",
      "Accuracy (LLM with Examples): 15.05%\n",
      "Precision (LLM with Examples): 0.41\n",
      "Recall (LLM with Examples): 0.15\n",
      "F1 Score (LLM with Examples): 0.22\n",
      "\n",
      "Accuracy (SpaCy NER): 50.00%\n",
      "Precision (SpaCy NER): 0.44\n",
      "Recall (SpaCy NER): 0.50\n",
      "F1 Score (SpaCy NER): 0.47\n"
     ]
    }
   ],
   "source": [
    "# evaluation with precision, recall, and F1 score\n",
    "def evaluation_metrics(manual_categories, llm_categories):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    total_ground_truth = 0\n",
    "\n",
    "    for llm_entities, manual_entities in zip(manual_categories, llm_categories):\n",
    "\n",
    "        manual_entities_only = {ent for ent in manual_entities}\n",
    "        llm_entities_only = {ent for ent in llm_entities}\n",
    "\n",
    "        total_ground_truth += len(manual_entities_only)\n",
    "\n",
    "        # true positives, false positives, and false negatives\n",
    "        true_positives += len(manual_entities_only.intersection(llm_entities_only))\n",
    "        false_positives += len(llm_entities_only - manual_entities_only)\n",
    "        false_negatives += len(manual_entities_only - llm_entities_only)\n",
    "\n",
    "    # Precision, Recall, and F1 Score\n",
    "    precision = (\n",
    "        true_positives / (true_positives + false_positives)\n",
    "        if (true_positives + false_positives) > 0\n",
    "        else 0\n",
    "    )\n",
    "    recall = (\n",
    "        true_positives / (true_positives + false_negatives)\n",
    "        if (true_positives + false_negatives) > 0\n",
    "        else 0\n",
    "    )\n",
    "    f1 = (\n",
    "        2 * (precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    accuracy = (\n",
    "        (true_positives / total_ground_truth) * 100 if total_ground_truth > 0 else 0\n",
    "    )\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "manual_annotations_categories = [\n",
    "    categories for categories in manual_annotations.values()\n",
    "]\n",
    "\n",
    "# Evaluation for LLM without examples\n",
    "accuracy_llm, precision_llm, recall_llm, f1_llm = evaluation_metrics(\n",
    "    manual_annotations_categories, llm_categories\n",
    ")\n",
    "\n",
    "# Evaluation for LLM with examples\n",
    "(\n",
    "    accuracy_llm_with_examples,\n",
    "    precision_llm_with_examples,\n",
    "    recall_llm_with_examples,\n",
    "    f1_llm_with_examples,\n",
    ") = evaluation_metrics(manual_annotations_categories, llm_with_examples_categories)\n",
    "\n",
    "# Evaluation for SpaCy NER\n",
    "(\n",
    "    accuracy_spacy_ner,\n",
    "    precision_spacy_ner,\n",
    "    recall_spacy_ner,\n",
    "    f1_spacy_ner,\n",
    ") = evaluation_metrics(manual_annotations_categories, spacy_results_to_compare[:20])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy (LLM without Examples): {accuracy_llm:.2f}%\")\n",
    "print(f\"Precision (LLM without Examples): {precision_llm:.2f}\")\n",
    "print(f\"Recall (LLM without Examples): {recall_llm:.2f}\")\n",
    "print(f\"F1 Score (LLM without Examples): {f1_llm:.2f}\\n\")\n",
    "\n",
    "print(f\"Accuracy (LLM with Examples): {accuracy_llm_with_examples:.2f}%\")\n",
    "print(f\"Precision (LLM with Examples): {precision_llm_with_examples:.2f}\")\n",
    "print(f\"Recall (LLM with Examples): {recall_llm_with_examples:.2f}\")\n",
    "print(f\"F1 Score (LLM with Examples): {f1_llm_with_examples:.2f}\\n\")\n",
    "\n",
    "print(f\"Accuracy (SpaCy NER): {accuracy_spacy_ner:.2f}%\") # <- winner\n",
    "print(f\"Precision (SpaCy NER): {precision_spacy_ner:.2f}\")\n",
    "print(f\"Recall (SpaCy NER): {recall_spacy_ner:.2f}\")\n",
    "print(f\"F1 Score (SpaCy NER): {f1_spacy_ner:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key Findings\n",
    "- True Positives (TP): Correctly identified entities.\n",
    "- False Positives (FP): Incorrect predictions (e.g., \"humans\" classified as \"PERSON\" instead of \"NORP\").\n",
    "- False Negatives (FN): Missed entities (e.g., LLM without examples omitted several entities).\n",
    "-- \n",
    "##### Common Issues\n",
    "Misclassifications:\n",
    "- Confusion between entity types:\n",
    "    - Example: \"American militia\" classified as \"GROUP\" instead of \"NORP\".\n",
    "    - Example: SpaCy classified \"Toto\" as \"ORG\" instead of \"PERSON\".\n",
    "--\n",
    "##### Omissions:\n",
    "LLM without examples missed several entities entirely (e.g., empty predictions in some cases).\n",
    "- Redundant Predictions:\n",
    "    - Unnecessary classifications:\n",
    "        - Example: \"small doses\" predicted as \"QUANTITY\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. Performance Comparison: LLM-based NER vs. Traditional Approaches\n",
    "\n",
    "- LLM-based NER can achieve competitive accuracy, but in this analysis, the accuracy of the LLM without examples was 26.47%, and with examples, it was 25.00%. Traditional models like SpaCy often outperform LLMs in accuracy. LLMs are more resource-intensive and slower, making them less suitable for resource-constrained environments.\n",
    "\n",
    "2. Effective Prompting Strategy\n",
    "- Contextual prompting is the most effective strategy for NER and classification tasks, as it provides relevant context that guides the model's predictions.\n",
    "\n",
    "3. Limitations and Biases of LLMs\n",
    "- LLMs may struggle with domain-specific terminology and can produce inconsistent results. They also inherit biases from training data, which can affect predictions.\n",
    "\n",
    "4. Recommendations\n",
    "- Traditional NER: Best for high accuracy, speed, and low resource usage.\n",
    "LLM-based Approaches: Suitable for tasks requiring flexibility and nuanced understanding, but they are time-consuming and resource-intensive when run locally.\n",
    "Conclusion\n",
    "Choose traditional NER for efficiency and accuracy, while LLMs are better for complex tasks despite their high resource demands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

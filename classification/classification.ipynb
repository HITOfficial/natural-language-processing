{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bochnak/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from p_tqdm import p_map\n",
    "# Load model directly\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, pipeline\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the FIQA-PL dataset that was used in lab 1 and lab lab 2 (so we need the passages, the questions and their relations).\n",
    "\n",
    "Got confiused with FIQA-PL, because indeed it doesn't provide relations between questions and corpus\n",
    "instead used truthful_qa (small english dataset with questions with multiple possible answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['type', 'category', 'question', 'best_answer', 'correct_answers', 'incorrect_answers', 'source'],\n",
       "        num_rows: 817\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset of positive and negative sentence pairs.\n",
    "\n",
    "- In each pair the first element is a question and the second element is a passagei, i.e. \"{question} {separator} {passage}\", where separator should be a separator taken from the model's tokenizer.\n",
    "- Use the relations to mark the positive pairs (i.e. pairs where the question is answered by the passage).\n",
    "- Use your own strategy to mark negative pairs (i.e. you can draw the negative examples, but there are better strategies to define the negative examples). The number of negative examples should be larger than the number of positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs, negative_pairs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separator = tokenizer.sep_token or \"[SEP]\"\n",
    "separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(question, answers):\n",
    "    pairs = []\n",
    "    for answer in answers:\n",
    "        pairs.append(question + ' ' + separator + ' ' + answer)\n",
    "    return pairs\n",
    "\n",
    "positive_pairs = [generate_pairs(q, a) for q, a in zip(ds['validation']['question'], ds['validation']['correct_answers'])]\n",
    "# positive_pairs = p_map(generate_pairs, ds['validation']['question'], ds['validation']['correct_answers']) # p_map is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#  72 mln downloads last month Sentence Transformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def get_top_n_similar_answers(correct_answers, incorrect_answers, candidate_incorrect_answers, top_n=None):\n",
    "\n",
    "    reference_vectors = model.encode(incorrect_answers)\n",
    "    candidate_vectors = model.encode(candidate_incorrect_answers)\n",
    "    \n",
    "    combined_reference_vector = np.mean(reference_vectors, axis=0)\n",
    "    \n",
    "    # cosine similarities between the combined reference vector and all candidate answers\n",
    "    cos_similarities = cosine_similarity([combined_reference_vector], candidate_vectors).flatten()\n",
    "    \n",
    "    # omit correct answers from candidates\n",
    "    filtered_indices = [\n",
    "        idx for idx, candidate in enumerate(candidate_incorrect_answers) \n",
    "        if candidate not in correct_answers\n",
    "    ]\n",
    "\n",
    "    \n",
    "    filtered_similarities = cos_similarities[filtered_indices]\n",
    "    sorted_indices = np.argsort(filtered_similarities)[::-1][:top_n]\n",
    "    \n",
    "    top_n_candidates = [candidate_incorrect_answers[filtered_indices[idx]] for idx in sorted_indices]\n",
    "\n",
    "    return top_n_candidates\n",
    "\n",
    "\n",
    "\n",
    "def generate_incorrect_answers(index):\n",
    "    correct_answers = ds['validation']['correct_answers']\n",
    "    incorrect_answers = ds['validation']['incorrect_answers']\n",
    "    candidate_incorrect_answers = [\n",
    "        incorrect for idx, incorrect in enumerate(incorrect_answers) if idx != index\n",
    "    ]\n",
    "    top_n = 3\n",
    "    # flaten the list of candidate incorrect answers\n",
    "    candidate_incorrect_answers = [item for sublist in candidate_incorrect_answers for item in sublist]\n",
    "    # Get top N most similar incorrect answers\n",
    "    similar_incorrect_answers = get_top_n_similar_answers(\n",
    "        correct_answers[index],\n",
    "        incorrect_answers[index],\n",
    "        candidate_incorrect_answers,\n",
    "        top_n\n",
    "    )\n",
    "    return similar_incorrect_answers + incorrect_answers[index]\n",
    "\n",
    "\n",
    "ds_len = len(ds['validation']['correct_answers'])\n",
    "\n",
    "# new_incorrect_answers = p_map(\n",
    "#     generate_incorrect_answers,\n",
    "#     range(ds_len)\n",
    "# )\n",
    "\n",
    "# p_map stopped working\n",
    "\n",
    "new_incorrect_answers = [\n",
    "    generate_incorrect_answers(i) for i in range(ds_len)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pairs = [generate_pairs(q, a) for q, a in zip(ds['validation']['question'], new_incorrect_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_negative_pairs = [item for sublist in negative_pairs for item in sublist]\n",
    "flatten_positive_pairs = [item for sublist in positive_pairs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive pairs 2600 and negative pairs 5437\n"
     ]
    }
   ],
   "source": [
    "print(f'positive pairs {len(flatten_positive_pairs)} and negative pairs {len(flatten_negative_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added three incorrect answers to each questions derived from the incorrect answers of other questions, utilizing cosine similarity to ensure relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset from point 2 should be split into training, evaluation and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_pairs = positive_pairs + negative_pairs\n",
    "np.random.shuffle(all_pairs)\n",
    "\n",
    "#  split 70% train, 15% eval, 15% test\n",
    "total_size = len(all_pairs)\n",
    "train_size = int(0.7 * total_size)\n",
    "eval_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - eval_size  # Ensure all data is used\n",
    "\n",
    "# Create labels: 1 for positive pairs, 0 for negative pairs\n",
    "labels = [1] * len(positive_pairs) + [0] * len(negative_pairs)\n",
    "\n",
    "# Create a dataset\n",
    "data = {'text': all_pairs, 'label': labels}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
    "test_dataset = dataset.select(range(train_size + eval_size, total_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a text classifier using the Transformers library\n",
    "distinguishes between the positive and the negative pairs. To make the process manageable use models of size base and a runtime providing GPU/TPU acceleration. Consult the discussions related to fine-tuning Transformer models to select sensible set of parameters. You can also run several trainings with different hyper-parameters, if you have access to large computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar incorrect candidates: ['Vienna', 'Hambourg', 'Freedom', 'hamburger', 'I am a big boy']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

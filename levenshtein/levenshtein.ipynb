{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from spacy.lang.pl import Polish\n",
    "from p_tqdm import p_map\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the corpus from exercise no. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 57638\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"corpus\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SpaCy tokenizer API to tokenize the text in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Polish()\n",
    "document = ds[\"corpus\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for doc in nlp.pipe(document):\n",
    "    tokenized.append([token.text.lower() for token in doc if not token.is_punct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute frequency list for each of the processed files, and aggregate the result to obtain one global frequency list. This frequency list gives you unigram statistics of the words appearing in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:19<00:00, 2924.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_frequencies(t):\n",
    "    return Counter(t)\n",
    "# parallelize the counting\n",
    "frequencies = p_map(count_frequencies, tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:30<00:00,  9.03s/it]\n"
     ]
    }
   ],
   "source": [
    "def sum_counters(counter_list):\n",
    "    return sum(counter_list, Counter())\n",
    "\n",
    "chunked_results_10 = p_map(sum_counters, [frequencies[i::10] for i in range(10)]) # split the data into 10 chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "chunked_results_2 = p_map(sum_counters, [chunked_results_10[i::2] for i in range(2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = sum_counters(chunked_results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w', 175366), ('nie', 131482), ('i', 126839), ('na', 119047), ('to', 116468)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a distortion function to the queries part of the corpus. In each query draw randomly one word and change one letter in the word to some other letter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort_query_with_one_random_word_letter(query):\n",
    "    words = query.split()\n",
    "    # For one word in the query, one letter is going to be changed to a random one if the word is longer than 2\n",
    "    indices_of_words_to_change = [i for i in range(len(words)) if len(words[i]) > 2]\n",
    "    \n",
    "    if not indices_of_words_to_change:\n",
    "        return query  # Return the original query if no words can be changed\n",
    "\n",
    "    index_of_word_to_change = np.random.choice(indices_of_words_to_change)\n",
    "    word = words[index_of_word_to_change]\n",
    "    index = np.random.choice(len(word))\n",
    "    new_letter = random.choice(\"abcdefghijklmnopqrstuvwxyz\".replace(word[index], \"\"))\n",
    "    \n",
    "    new_word = word[:index] + new_letter + word[index + 1:]    \n",
    "    words[index_of_word_to_change] = new_word\n",
    "    \n",
    "    return ' '.join(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:14<00:00, 4038.28it/s]\n"
     ]
    }
   ],
   "source": [
    "distorted_queries = p_map(distort_query_with_one_random_word_letter, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(scores, k):\n",
    "    scores = scores[:k]\n",
    "    return sum(score / np.log2(idx + 2) for idx, score in enumerate(scores))\n",
    "\n",
    "# nDCG@k\n",
    "def ndcg_at_k(relevance_scores, k=10):\n",
    "    ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "    dcg = dcg_at_k(relevance_scores, k)\n",
    "    idcg = dcg_at_k(ideal_scores, k)\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:40<00:00, 1440.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nDCG@10 for distorted queries: 0.4999274433971518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "relevance_scores = [random.choices([0, 1], k=len(query)) for query in distorted_queries]\n",
    "ndcg_scores = p_map(ndcg_at_k, relevance_scores)\n",
    "\n",
    "# Average nDCG@10 \n",
    "avg_ndcg_10 = np.mean(ndcg_scores)\n",
    "print(f\"Average nDCG@10 for distorted queries: {avg_ndcg_10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

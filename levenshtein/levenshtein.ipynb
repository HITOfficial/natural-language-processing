{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bochnak/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from spacy.lang.pl import Polish\n",
    "from p_tqdm import p_map\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the corpus from exercise no. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 57638\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"corpus\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SpaCy tokenizer API to tokenize the text in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Polish()\n",
    "document = ds[\"corpus\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for doc in nlp.pipe(document):\n",
    "    tokenized.append([token.text.lower() for token in doc if not token.is_punct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute frequency list for each of the processed files, and aggregate the result to obtain one global frequency list. This frequency list gives you unigram statistics of the words appearing in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:20<00:00, 2838.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_frequencies(t):\n",
    "    return Counter(t)\n",
    "# parallelize the counting\n",
    "frequencies = p_map(count_frequencies, tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:29<00:00,  8.90s/it]\n"
     ]
    }
   ],
   "source": [
    "def sum_counters(counter_list):\n",
    "    return sum(counter_list, Counter())\n",
    "\n",
    "chunked_results_10 = p_map(sum_counters, [frequencies[i::10] for i in range(10)]) # split the data into 10 chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "chunked_results_2 = p_map(sum_counters, [chunked_results_10[i::2] for i in range(2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = sum_counters(chunked_results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w', 175366), ('nie', 131482), ('i', 126839), ('na', 119047), ('to', 116468)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a distortion function to the queries part of the corpus. In each query draw randomly one word and change one letter in the word to some other letter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort_query_with_one_random_word_letter(query):\n",
    "    words = query.split()\n",
    "    # For one word in the query, one letter is going to be changed to a random one if the word is longer than 2\n",
    "    indices_of_words_to_change = [i for i in range(len(words)) if len(words[i]) > 2]\n",
    "    \n",
    "    if not indices_of_words_to_change:\n",
    "        return query  # Return the original query if no words can be changed\n",
    "\n",
    "    index_of_word_to_change = np.random.choice(indices_of_words_to_change)\n",
    "    word = words[index_of_word_to_change]\n",
    "    index = np.random.choice(len(word))\n",
    "    new_letter = random.choice(\"abcdefghijklmnopqrstuvwxyz\".replace(word[index], \"\"))\n",
    "    \n",
    "    new_word = word[:index] + new_letter + word[index + 1:]    \n",
    "    words[index_of_word_to_change] = new_word\n",
    "    \n",
    "    return ' '.join(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:13<00:00, 4328.87it/s]\n"
     ]
    }
   ],
   "source": [
    "distorted_queries = p_map(distort_query_with_one_random_word_letter, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(scores, k):\n",
    "    scores = scores[:k]\n",
    "    return sum(score / np.log2(idx + 2) for idx, score in enumerate(scores))\n",
    "\n",
    "# nDCG@k\n",
    "def ndcg_at_k(relevance_scores, k=10):\n",
    "    ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "    dcg = dcg_at_k(relevance_scores, k)\n",
    "    idcg = dcg_at_k(ideal_scores, k)\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:38<00:00, 1478.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nDCG@10 for distorted queries: 0.5007277011591081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "relevance_scores = [random.choices([0, 1], k=len(query)) for query in distorted_queries]\n",
    "ndcg_scores = p_map(ndcg_at_k, relevance_scores)\n",
    "\n",
    "# Average nDCG@10 \n",
    "avg_ndcg_10 = np.mean(ndcg_scores)\n",
    "print(f\"Average nDCG@10 for distorted queries: {avg_ndcg_10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Morfeusz (Binding dla Pythona) and use it to find all words from the queries that do not appear in that dictionary. Only these words should be corected in the next step.\n",
    "Mac User, trying alternative spacy and pl_core_news_sm  https://spacy.io/models/pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_nlp= spacy.load(\"pl_core_news_sm\")\n",
    "\n",
    "def in_dictionary(word):\n",
    "    return word.lower() in vocab_nlp.vocab.strings\n",
    "\n",
    "\n",
    "def word_if_not_in_dictionary(word):\n",
    "    if not in_dictionary(word):\n",
    "        return word\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184625/184625 [00:34<00:00, 5407.91it/s]\n"
     ]
    }
   ],
   "source": [
    "words_not_in_dictionary = p_map(word_if_not_in_dictionary, final_result.keys())\n",
    "# filter empty strings\n",
    "words_not_in_dictionary = [word for word in words_not_in_dictionary if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fdic',\n",
       " 'rezygnujesz',\n",
       " 'bankomatów',\n",
       " 'wyceniane',\n",
       " 'nomenklaturze',\n",
       " 'końskiej',\n",
       " 'malują',\n",
       " 'aktualizacje',\n",
       " 'flashnews',\n",
       " 'sergei']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_not_in_dictionary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Levenshtein distance and the frequency list, to determine the most probable correction of the words in the queries that were identified as invalid.\n",
    " (Note: You don't have to apply the distance directly. Any method that is more efficient than scanning the dictionary will be appreciated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 \n",
    "            deletions = current_row[j - 1] + 1  \n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def manual_fuzzy_match(query, dictionary):\n",
    "    max_ratio = 0\n",
    "    best_match = \"\"\n",
    "    \n",
    "    # Convert query and dictionary words to lowercase\n",
    "    query_lower = query.lower()\n",
    "    dict_words_lower = [word.lower() for word in dictionary]\n",
    "    \n",
    "    for word in dict_words_lower:\n",
    "        ratio = sum(c1 == c2 for c1, c2 in zip(query_lower, word)) / min(len(query_lower), len(word))\n",
    "        \n",
    "        if ratio > max_ratio:\n",
    "            max_ratio = ratio\n",
    "            best_match = word\n",
    "    \n",
    "    return best_match\n",
    "\n",
    "def correct_words(words_not_in_dictionary, dictionary, frequency_list):\n",
    "    corrected_words = {}\n",
    "\n",
    "    # Sort dictionary by frequency, using 0 as the default value for missing frequencies\n",
    "    sorted_dict = sorted(dictionary, key=lambda word: frequency_list.get(word, 0), reverse=True)\n",
    "    \n",
    "    for word in words_not_in_dictionary:\n",
    "        # Find the closest match in the dictionary\n",
    "        closest_match = manual_fuzzy_match(word, sorted_dict)\n",
    "        \n",
    "        # Calculate Levenshtein distance\n",
    "        distance = levenshtein_distance(word, closest_match)\n",
    "        \n",
    "        # Check if the distance is within an acceptable threshold\n",
    "        if distance <= 2:  # Adjust this threshold as needed\n",
    "            corrected_words[word] = closest_match\n",
    "        else:\n",
    "            # If no close match, use the original word\n",
    "            corrected_words[word] = word\n",
    "    \n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {\n",
    "    w: c \n",
    "    for w, c in final_result.items()\n",
    "    if not in_dictionary(w)\n",
    "}\n",
    "\n",
    "def get_word_rank(w):\n",
    "    return word_frequencies[w]\n",
    "\n",
    "def get_suggestions(w, num_suggestions=1, max_distance=1):\n",
    "    return sorted(generate_candidate_words(w, max_distance), key=get_word_rank)[:num_suggestions]\n",
    "\n",
    "def generate_candidate_words(w, max_distance):\n",
    "    candidate_set = {w}\n",
    "    for _ in range(max_distance):\n",
    "        edits = (generate_single_edit(c) for c in candidate_set)\n",
    "        candidate_set = candidate_set.union(*edits)\n",
    "    return filter_known_candidates(candidate_set)\n",
    "\n",
    "def filter_known_candidates(words):\n",
    "    return {w for w in words if w in word_frequencies}\n",
    "\n",
    "def generate_single_edit(w):\n",
    "    alphabet = 'aąbcćdeęfghijklłmnoópqrsśtuvwxyz'\n",
    "    splits = [(w[:i], w[i:]) for i in range(len(w) + 1)]\n",
    "    deletions = [L + R[1:] for L, R in splits if R]\n",
    "    transpositions = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replacements = [L + c + R[1:] for L, R in splits if R for c in alphabet]\n",
    "    insertions = [L + c + R for L, R in splits for c in alphabet]\n",
    "    return set(deletions + transpositions + replacements + insertions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: fdic candidate: fslic, cdsc, dlc, fei, fvif, ficc, uic, frac, bdc, idfc, idc, fdap, fica, ic, blic, fid, feit, dif, wdc, fmcc, fide, fix, fice, fsia, odc, faik, eic, ftc, pfic, fxc, bdac, fdia, cdic, fait, fmac, flip, xic, fib, fdny, epic, dnc, foia, dice, ivic, cdi, fcmc, fdgh, fcac, cdc, sdic, icic, fdi, fnic, flac, bric, roic, dbc, dtic, fdic, ffiec, favc, naic, fico, ific, fido, fdca, oeic, dip, spic, hdfc, fnbc, fdica, foil, edlc, fpsc, mdc, diy, dich, fomc, fwiw, chic, dsc, fda, fyi, feie, fd, dc, dtc, saic, asic, fdr, fdo, fail, esic, fris, fmdi, cbic\n",
      "word: rezygnujesz candidate: rezygnujesz\n",
      "word: bankomatów candidate: bankomatowy, bankomaty, bankomatowa, bankomatowe, bankomatem, bankomatową, bankomatów\n",
      "word: wyceniane candidate: wyceniały, wycenianym, wyceniała, wycenianej, wyceniam, wyceniali, wyceniona, wyceniają, wyceniani, wyceniaj, wycenione, wycenianego, wycenioną, wyceniana, wyceniamy, wycenianiu, wyceniania, wyceniany, wyceniał, wyceniono, wyceniane, wycenianie\n",
      "word: nomenklaturze candidate: nomenklaturze, nomenklaturę\n",
      "word: końskiej candidate: koński, końskiej\n",
      "word: malują candidate: masuje, maluję, mapują, mdleją, tagują, skalują, kasują, zmaleją, malujmy, waliją, zalają, masują, malutką, smarują, marnują, malują, maluj, rolują, mapuje, kauują, ratują, malujący\n",
      "word: aktualizacje candidate: aktualizacją, aktualizacjom, aktualizacje, aktualizację\n",
      "word: flashnews candidate: flashnews\n",
      "word: sergei candidate: seigel, sergen, cersei, verge, eroei, serve, server, series, selge, surger, surge, merger, sergey, sergei, serze, merge, serves\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevances = []\n",
    "for word in words_not_in_dictionary[:10]:\n",
    "    corrs = generate_candidate_words(word, 2)\n",
    "    corrs_str = \", \".join(corrs)\n",
    "    print(f\"word: {word} candidate: {corrs_str}\")\n",
    "    retrieved_relevance = [1 if in_dictionary(cor) else 0 for cor in corrs]\n",
    "    retrieved_relevances.append(retrieved_relevance)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute nDCG@10 for your implementation of the spelling correction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: fdic nDCG@10 for candidates: 0.0000\n",
      "word: rezygnujesz nDCG@10 for candidates: 0.0000\n",
      "word: bankomatów nDCG@10 for candidates: 0.0000\n",
      "word: wyceniane nDCG@10 for candidates: 0.0000\n",
      "word: nomenklaturze nDCG@10 for candidates: 0.0000\n",
      "word: końskiej nDCG@10 for candidates: 0.0000\n",
      "word: malują nDCG@10 for candidates: 0.0000\n",
      "word: aktualizacje nDCG@10 for candidates: 0.0000\n",
      "word: flashnews nDCG@10 for candidates: 0.0000\n",
      "word: sergei nDCG@10 for candidates: 0.0000\n",
      "\n",
      "No correct candidate found\n"
     ]
    }
   ],
   "source": [
    "for word, retrieved_relevance in zip(words_not_in_dictionary[:10], retrieved_relevances):\n",
    "    ndcg_at_10 = ndcg_at_k(retrieved_relevance, k=10)\n",
    "    print(f\"word: {word} nDCG@10 for candidates: {ndcg_at_10:.4f}\")\n",
    "\n",
    "if sum(retrieved_relevance) == 0:\n",
    "    print(\"\\nNo correct candidate found\") # vocabulary data didn't contain correct candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ElasticSearch's fuzzy match and compute nDCG@10 for this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = \"http://localhost:9200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'node-1',\n",
       " 'cluster_name': 'my-application-cluster',\n",
       " 'cluster_uuid': 'CKjkeoDKR3G-PRs-eQpY9w',\n",
       " 'version': {'number': '8.15.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'docker',\n",
       "  'build_hash': '98adf7bf6bb69b66ab95b761c9e5aadb0bb059a3',\n",
       "  'build_date': '2024-09-19T10:06:03.564235954Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '9.11.1',\n",
       "  'minimum_wire_compatibility_version': '7.17.0',\n",
       "  'minimum_index_compatibility_version': '7.0.0'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Elasticsearch(es_url)\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'fiqa_pl_index_words' already exists.\n"
     ]
    }
   ],
   "source": [
    "index_name = \"fiqa_pl_index_words\"\n",
    "\n",
    "\n",
    "index_body = {   \n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if not client.indices.exists(index=index_name):\n",
    "    client.indices.create(index=index_name, body=index_body)\n",
    "    print(f\"Index '{index_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_data = [\n",
    "    {\n",
    "        \"_index\": index_name,\n",
    "        \"_id\": key,\n",
    "        \"_source\": {\n",
    "            \"word\": key\n",
    "        }\n",
    "    }\n",
    "    for key in final_result.keys()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184625, [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.bulk(client, bulk_data) # bulk load the data into the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: fdic candidate: fdic, cdic, fnic, sdic, fdica, fdia, fdi, eric, fico, fica\n",
      "word: rezygnujesz candidate: rezygnujesz, zrezygnujesz, rezygnuje\n",
      "word: bankomatów candidate: bankomatów, bankomatowa, bankomatową, bankomatowy, bankomatowe, bankomatem, parkomatów, bankomatu, bankomaty, bankomat\n",
      "word: wyceniane candidate: wyceniane, wycenione, wyceniana, wyceniany, wycenianej, wycenianie, wyceniani, wyceniają, wyceniasz, wymieniane\n",
      "word: nomenklaturze candidate: nomenklaturze, nomenklaturą, nomenklaturę, nomenklatury\n",
      "word: końskiej candidate: końskiej, końskie, morskiej, końskiego, boskiej, koński, morskiej).cienka\n",
      "word: malują candidate: malują, maleją, maluje, mapują, maluję, masują, malując, maluj, walutą, maleje\n",
      "word: aktualizacje candidate: aktualizacje, aktualizacja, aktualizacji, aktualizację, aktualizacją, aktualizacjom, aktualizuje, aktualizację\"\"\"\"\"\"\"tak, premię?(aktualizacja\n",
      "word: flashnews candidate: flashnews\n",
      "word: sergei candidate: sergei, sergey, sergen, series, serwer, sercem, serami, szeregi, server, merger\n"
     ]
    }
   ],
   "source": [
    "for word in words_not_in_dictionary[:10]:\n",
    "    response = client.search(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"word\": {\n",
    "                        \"query\": word,\n",
    "                        \"fuzziness\": 2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    corrs = [hit[\"_source\"][\"word\"] for hit in response.get(\"hits\", {}).get(\"hits\", [])]\n",
    "    corrs_str = \", \".join(corrs) if corrs else \"No candidates found\"\n",
    "\n",
    "    print(f\"word: {word} candidate: {corrs_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: fdic nDCG@10: 0.0000\n",
      "word: rezygnujesz nDCG@10: 0.6934\n",
      "word: bankomatów nDCG@10: 0.4401\n",
      "word: wyceniane nDCG@10: 0.3618\n",
      "word: nomenklaturze nDCG@10: 0.6509\n",
      "word: końskiej nDCG@10: 0.7606\n",
      "word: malują nDCG@10: 0.6968\n",
      "word: aktualizacje nDCG@10: 0.6871\n",
      "word: flashnews nDCG@10: 0.0000\n",
      "word: sergei nDCG@10: 0.5434\n",
      "\n",
      "Average nDCG@10: 0.4834\n"
     ]
    }
   ],
   "source": [
    "ndcg_results = []\n",
    "\n",
    "for word in words_not_in_dictionary[:10]:\n",
    "    response = client.search(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"word\": {\n",
    "                        \"query\": word,\n",
    "                        \"fuzziness\": \"AUTO\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    retrieved_relevances = []\n",
    "    for hit in response.get(\"hits\", {}).get(\"hits\", []):\n",
    "        if hit[\"_source\"][\"word\"] in vocab_nlp.vocab.strings:\n",
    "            retrieved_relevances.append(1)\n",
    "        else:\n",
    "            retrieved_relevances.append(0)\n",
    "\n",
    "    # Compute nDCG@10\n",
    "    ndcg_at_10 = ndcg_at_k(retrieved_relevances)\n",
    "    ndcg_results.append(ndcg_at_10)\n",
    "    print(f\"word: {word} nDCG@10: {ndcg_at_10:.4f}\")\n",
    "    \n",
    "average_ndcg = (sum(ndcg_results) / len(ndcg_results))\n",
    "print(f\"\\nAverage nDCG@10: {average_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the results of baseline with the 2 implemented methods. Take into account the nDCG score and the performance of the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Baseline Method (Elasticsearch)**:\n",
    "   - **Average nDCG@10**: **0.4022**\n",
    "   - Some words had high relevance (e.g., \"końskiej\": 0.7606), while others had none (e.g., \"fdic\": 0.0000).\n",
    "\n",
    "2. **Levenshtein Method**:\n",
    "   - **Average nDCG@10**: **0.0000**\n",
    "   - No relevant candidates found for any word.\n",
    "\n",
    "### Conclusion\n",
    "The **Elasticsearch method** outperformed the **Levenshtein method** significantly in nDCG scores and relevance retrieval. I would have more realistic resluts for Levenhstein Method if I would have more valid polish vocab dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Correction Using GPT-4o, Elasticsearch, and Levenshtein Distance\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "1. **Distribution of Words in the Corpus**:\n",
    "   - The distribution of words in the corpus significantly affects the performance of both methods. Underrepresented words can lead to lower correction accuracy. GPT-4o's contextual understanding allows it to generate more relevant corrections, even for less common words.\n",
    "\n",
    "2. **Performance of Levenshtein Compared to Elasticsearch**:\n",
    "   -  GPT-4o outperformed the Levenshtein method in generating corrections. While Elasticsearch demonstrated superior retrieval capabilities, GPT-4o excelled in understanding and correcting queries contextually.\n",
    "\n",
    "3. **Results of Levenshtein Compared to Elasticsearch**:\n",
    "   - GPT-4 produced corrections with higher relevance and accuracy compared to the Levenshtein method. However, Elasticsearch remains a robust tool for retrieving relevant documents based on corrected queries, while GPT-4 is better suited for generating meaningful corrections.\n",
    "\n",
    "4. **Validity of the Obtained Corrections**:\n",
    "   - The corrections from GPT-4o were generally valid and contextually appropriate. In contrast, the Levenshtein method often yielded corrections that were syntactically correct but semantically irrelevant, highlighting the importance of context in query correction.\n",
    "\n",
    "5. **Ability of an LLM to Fix Invalid Queries**:\n",
    "   - The analysis demonstrated that an LLM like GPT-4o effectively fixes invalid queries. It showed a strong capacity to understand the intent behind distorted queries and provide meaningful corrections, unlike the Levenshtein method, which lacked contextual awareness.\n",
    "\n",
    "### Summary\n",
    "Overall, GPT-4o proved to be a more effective tool for query correction compared to the Levenshtein distance method, particularly in terms of contextual understanding and relevance. The combination of LLMs for correction and Elasticsearch for retrieval could enhance overall performance in handling distorted queries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

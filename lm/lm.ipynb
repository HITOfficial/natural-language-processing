{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bochnak/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM, pipeline\n",
    "from transformers.models.bert import BertForMaskedLM\n",
    "from transformers.models.gpt2 import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download three Polish models from the Huggingface repository.\n",
    "These should be regular language models, which were not fine-tuned. E.g. HerBERT and papuGaPT2 are good examples. You can also try using Bielik for that, but make sure you are using the model via Transformers API, not GUI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From v4.50 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"HerBERT\": {\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\"),\n",
    "        \"model\": AutoModelForMaskedLM.from_pretrained(\"allegro/herbert-base-cased\"),\n",
    "        \"pipeline\": None,\n",
    "    },\n",
    "    \"papuGaPT2\": {\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"flax-community/papuGaPT2\"),\n",
    "        \"model\": AutoModelForCausalLM.from_pretrained(\"flax-community/papuGaPT2\"),\n",
    "        \"pipeline\": None,\n",
    "    },\n",
    "    \"Bielik\": {\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\"),\n",
    "        \"model\": AutoModelForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\"),\n",
    "        \"pipeline\": None,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devise a method to test if the langage model understands Polish cases.\n",
    " E.g. testing for nominal case could be expressed as \"Warszawa to najwiksze [MASK]\", and the masked word should be in nominative case. Create sentences for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline (fill-mask) successfully assigned for model: HerBERT\n",
      "Pipeline (text-generation) successfully assigned for model: papuGaPT2\n",
      "Pipeline (fill-mask) successfully assigned for model: Bielik\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_info in models.items():\n",
    "    model = model_info[\"model\"]\n",
    "\n",
    "    if isinstance(model, BertForMaskedLM):\n",
    "        fill_mask_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=model_info[\"tokenizer\"])\n",
    "        models[model_name][\"pipeline\"] = fill_mask_pipeline\n",
    "        print(f\"Pipeline (fill-mask) successfully assigned for model: {model_name}\")\n",
    "\n",
    "    elif isinstance(model, GPT2LMHeadModel):\n",
    "        text_generation_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=model_info[\"tokenizer\"],\n",
    "            max_new_tokens=2,\n",
    "            pad_token_id=model_info[\"tokenizer\"].eos_token_id,\n",
    "            eos_token_id=model_info[\"tokenizer\"].eos_token_id,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        models[model_name][\"pipeline\"] = text_generation_pipeline\n",
    "        print(f\"Pipeline (text-generation) successfully assigned for model: {model_name}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Model {model_name} is not compatible with the intended pipelines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing cases with HerBERT model:\n",
      "\n",
      "Case: nominative\n",
      "Prediction: Warszawa to najwiksze miasto . (Score: 0.8103939294815063)\n",
      "Prediction: Warszawa to najwiksze lotnisko . (Score: 0.08249125629663467)\n",
      "Prediction: Warszawa to najwiksze centrum . (Score: 0.026521336287260056)\n",
      "\n",
      "Case: genitive\n",
      "Prediction: To ci偶kie zadanie , potrzebuj troch wicej czasu . (Score: 0.4029397666454315)\n",
      "Prediction: To ci偶kie zadanie , potrzebuj troch wicej cierpliwoci . (Score: 0.11514861136674881)\n",
      "Prediction: To ci偶kie zadanie , potrzebuj troch wicej dowiadczenia . (Score: 0.09145424515008926)\n",
      "\n",
      "Case: dative\n",
      "Prediction: M贸j pies Zeus przyglda si temu . (Score: 0.3961878716945648)\n",
      "Prediction: M贸j pies Zeus przyglda si wszystkiemu . (Score: 0.13475088775157928)\n",
      "Prediction: M贸j pies Zeus przyglda si mu . (Score: 0.05179081857204437)\n",
      "\n",
      "Case: accusative\n",
      "Prediction: Wczoraj w stawie widziaem du偶 burz . (Score: 0.1044120118021965)\n",
      "Prediction: Wczoraj w stawie widziaem du偶 rzek . (Score: 0.0810621902346611)\n",
      "Prediction: Wczoraj w stawie widziaem du偶 dziur . (Score: 0.06021549180150032)\n",
      "\n",
      "Case: instrumental\n",
      "Prediction: Wczoraj podr贸偶owaem z moim synem . (Score: 0.21374928951263428)\n",
      "Prediction: Wczoraj podr贸偶owaem z moim m偶em . (Score: 0.13855023682117462)\n",
      "Prediction: Wczoraj podr贸偶owaem z moim koleg . (Score: 0.07405316829681396)\n",
      "\n",
      "Case: locative\n",
      "Prediction: Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich planach . (Score: 0.29985353350639343)\n",
      "Prediction: Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich dzieciach . (Score: 0.1757470667362213)\n",
      "Prediction: Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich problemach . (Score: 0.13204386830329895)\n",
      "\n",
      "Testing cases with papuGaPT2 model:\n",
      "\n",
      "Case: nominative\n",
      "Prediction: Warszawa to najwiksze r贸d na\n",
      "\n",
      "Case: genitive\n",
      "Prediction: To ci偶kie zadanie, potrzebuj troch wicej 锛\n",
      "\n",
      "Case: dative\n",
      "Prediction: M贸j pies Zeus przyglda si owaczom\n",
      "\n",
      "Case: accusative\n",
      "Prediction: Wczoraj w stawie widziaem du偶 r贸jk\n",
      "\n",
      "Case: instrumental\n",
      "Prediction: Wczoraj podr贸偶owaem z moim nemka\n",
      "\n",
      "Case: locative\n",
      "Prediction: Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich owach\n",
      "\n",
      "Testing cases with Bielik model:\n",
      "\n",
      "Case: nominative\n",
      "Prediction: Warszawa to najwiksze miasto. (Score: 0.9553059935569763)\n",
      "Prediction: Warszawa to najwiksze wojew贸dztwo. (Score: 0.0065080937929451466)\n",
      "Prediction: Warszawa to najwiksze lotnisko. (Score: 0.005074796266853809)\n",
      "\n",
      "Case: genitive\n",
      "Prediction: To ci偶kie zadanie, potrzebuj troch wicej czasu. (Score: 0.6750841736793518)\n",
      "Prediction: To ci偶kie zadanie, potrzebuj troch wicej pracy. (Score: 0.020163556560873985)\n",
      "Prediction: To ci偶kie zadanie, potrzebuj troch wicej pienidzy. (Score: 0.018410973250865936)\n",
      "\n",
      "Case: dative\n",
      "Prediction: M贸j pies Zeus przyglda si temu. (Score: 0.16015353798866272)\n",
      "Prediction: M贸j pies Zeus przyglda si tobie. (Score: 0.07319647818803787)\n",
      "Prediction: M贸j pies Zeus przyglda si jej. (Score: 0.0676044449210167)\n",
      "\n",
      "Case: accusative\n",
      "Prediction: Wczoraj w stawie widziaem du偶 ryb. (Score: 0.7800177335739136)\n",
      "Prediction: Wczoraj w stawie widziaem du偶 dziewczynk. (Score: 0.017225343734025955)\n",
      "Prediction: Wczoraj w stawie widziaem du偶 rzecz. (Score: 0.011612358503043652)\n",
      "\n",
      "Case: instrumental\n",
      "Prediction: Wczoraj podr贸偶owaem z moim ojcem. (Score: 0.32932519912719727)\n",
      "Prediction: Wczoraj podr贸偶owaem z moim m偶em. (Score: 0.1357676386833191)\n",
      "Prediction: Wczoraj podr贸偶owaem z moim bratem. (Score: 0.09200859814882278)\n",
      "\n",
      "Case: locative\n",
      "Prediction: Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich dzieciach. (Score: 0.2987850606441498)\n",
      "Prediction: Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich problemach. (Score: 0.14421924948692322)\n",
      "Prediction: Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich planach. (Score: 0.07317749410867691)\n"
     ]
    }
   ],
   "source": [
    "masks = {\n",
    "    \"HerBERT\": \"<mask>.\",\n",
    "    \"Bielik\": \"[MASK].\",\n",
    "    \"papuGaPT2\" : \"\"\n",
    "}\n",
    "\n",
    "case_sentences = {\n",
    "    \"nominative\": \"Warszawa to najwiksze {mask}\",\n",
    "    \"genitive\": \"To ci偶kie zadanie, potrzebuj troch wicej {mask}\",\n",
    "    \"dative\": \"M贸j pies Zeus przyglda si {mask}\",\n",
    "    \"accusative\": \"Wczoraj w stawie widziaem du偶 {mask}\",\n",
    "    \"instrumental\": \"Wczoraj podr贸偶owaem z moim {mask}\",\n",
    "    \"locative\": \"Wczoraj po wywiad贸wce z moj mam rozmawialimy o moich {mask}\",\n",
    "}\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    if model_info[\"pipeline\"]:\n",
    "        print(f\"\\nTesting cases with {model_name} model:\")\n",
    "\n",
    "        mask = masks.get(model_name, \"<mask>\")\n",
    "\n",
    "        for case, sentence in case_sentences.items():\n",
    "            sentence_with_mask = sentence.format(mask=mask)\n",
    "            print(f\"\\nCase: {case}\")\n",
    "            outputs = model_info[\"pipeline\"](sentence_with_mask)\n",
    "            if model_name in [\"HerBERT\", \"Bielik\"]:\n",
    "                for output in outputs[:3]:\n",
    "                    print(f\"Prediction: {output['sequence']} (Score: {output['score']})\")\n",
    "            elif model_name == \"papuGaPT2\":\n",
    "                for output in outputs[:3]:\n",
    "                    print(f\"Prediction: {output['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devise a method to test long-range relationships such as gender.\n",
    "E.e. you can use two verbs with masculine and feminine gender, where one of the verbs is masked. Both verbs should have the same gender, assuming the subject is the same. Define at least 3 such sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing gender relationships with HerBERT model:\n",
      "Prediction: Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w , zanim ich leczy . (Score: 0.2329351007938385)\n",
      "Prediction: Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w , zanim ich odwiedzi . (Score: 0.1360275000333786)\n",
      "Prediction: Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w , zanim ich postawi . (Score: 0.13440170884132385)\n",
      "Prediction: Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je wykona . (Score: 0.38248470425605774)\n",
      "Prediction: Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je rozwiza . (Score: 0.10732978582382202)\n",
      "Prediction: Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je ukoczy . (Score: 0.09674196690320969)\n",
      "Prediction: Agnieszka bya zmczona , ale pomimo to dalej 偶ya . (Score: 0.16098259389400482)\n",
      "Prediction: Agnieszka bya zmczona , ale pomimo to dalej walczya . (Score: 0.14678335189819336)\n",
      "Prediction: Agnieszka bya zmczona , ale pomimo to dalej pracowaa . (Score: 0.124876007437706)\n",
      "\n",
      "Testing gender relationships with papuGaPT2 model:\n",
      "Prediction: Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w, zanim ich laryngolog\n",
      "Prediction: Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je owiza\n",
      "Prediction: Agnieszka bya zmczona, ale pomimo to dalej owaa\n",
      "\n",
      "Testing gender relationships with Bielik model:\n",
      "Prediction: Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w, zanim ich zobaczy. (Score: 0.21240124106407166)\n",
      "Prediction: Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w, zanim ich pozna. (Score: 0.08333886414766312)\n",
      "Prediction: Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w, zanim ich zabije. (Score: 0.04203284904360771)\n",
      "Prediction: Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je wykona. (Score: 0.234587162733078)\n",
      "Prediction: Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je rozwiza. (Score: 0.05233663320541382)\n",
      "Prediction: Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je ukoczy. (Score: 0.04892021417617798)\n",
      "Prediction: Agnieszka bya zmczona, ale pomimo to dalej 偶ya. (Score: 0.2349366545677185)\n",
      "Prediction: Agnieszka bya zmczona, ale pomimo to dalej spaa. (Score: 0.10330387949943542)\n",
      "Prediction: Agnieszka bya zmczona, ale pomimo to dalej pracowaa. (Score: 0.08858872950077057)\n"
     ]
    }
   ],
   "source": [
    "gender_sentences = [\n",
    "    \"Maria jest wietn lekark i zawsze dokadnie sucha swoich pacjent贸w, zanim ich {mask}\",\n",
    "    \"Andrzej spdzi du偶o czasu nad zadaniami zanim wszystkie je {mask}\",\n",
    "    \"Agnieszka bya zmczona, ale pomimo to dalej {mask}\",\n",
    "]\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    if model_info[\"pipeline\"]:\n",
    "        mask = masks.get(model_name, \"<mask>\")\n",
    "        print(f\"\\nTesting gender relationships with {model_name} model:\")\n",
    "        for sentence in gender_sentences:\n",
    "            sentence_with_mask = sentence.format(mask=mask)\n",
    "            outputs = model_info[\"pipeline\"](sentence_with_mask)\n",
    "            if model_name in [\"HerBERT\", \"Bielik\"]:\n",
    "                for output in outputs[:3]:  # Display top 3 predictions\n",
    "                    print(f\"Prediction: {output['sequence']} (Score: {output['score']})\")\n",
    "            elif model_name == \"papuGaPT2\":\n",
    "                for output in outputs[:3]:  # Display top 3 predictions\n",
    "                    print(f\"Prediction: {output['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if the model captures real-world knolwedge. For instance a sentence \"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\"\n",
    " checks if the model \"knows\" the description of water. Define at least 3 such sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing real-world knowledge with HerBERT model:\n",
      "Prediction: Stolic Polski jest Warszawa . (Score: 0.609553337097168)\n",
      "Prediction: Stolic Polski jest Krak贸w . (Score: 0.0991559699177742)\n",
      "Prediction: Stolic Polski jest Gdask . (Score: 0.08018141239881516)\n",
      "Prediction: Najwy偶sz g贸r Polski s Niemcy . (Score: 0.5421215891838074)\n",
      "Prediction: Najwy偶sz g贸r Polski s Polacy . (Score: 0.07686690241098404)\n",
      "Prediction: Najwy偶sz g贸r Polski s Austriacy . (Score: 0.040299177169799805)\n",
      "Prediction: Najlepszy w historii pikarz z Argentyny to . . (Score: 0.1165275052189827)\n",
      "Prediction: Najlepszy w historii pikarz z Argentyny to Francuz . (Score: 0.02994181029498577)\n",
      "Prediction: Najlepszy w historii pikarz z Argentyny to Polak . (Score: 0.025129610672593117)\n",
      "Prediction: Stolic Niemiec jest Berlin . (Score: 0.693077564239502)\n",
      "Prediction: Stolic Niemiec jest Monachium . (Score: 0.16482771933078766)\n",
      "Prediction: Stolic Niemiec jest Stuttgart . (Score: 0.1128314808011055)\n",
      "Prediction: Stolic Francji jest Pary偶 . (Score: 0.7776497602462769)\n",
      "Prediction: Stolic Francji jest Lyon . (Score: 0.13299959897994995)\n",
      "Prediction: Stolic Francji jest Bordeaux . (Score: 0.04730105400085449)\n",
      "Prediction: 2 wojn wiatow rozpoczli Niemcy . (Score: 0.7175590991973877)\n",
      "Prediction: 2 wojn wiatow rozpoczli Rosjanie . (Score: 0.06640005111694336)\n",
      "Prediction: 2 wojn wiatow rozpoczli Francuzi . (Score: 0.05092622712254524)\n",
      "Prediction: 2 wojn wiatow wygrali Niemcy . (Score: 0.7991869449615479)\n",
      "Prediction: 2 wojn wiatow wygrali Francuzi . (Score: 0.05936354771256447)\n",
      "Prediction: 2 wojn wiatow wygrali Brytyjczycy . (Score: 0.02893192321062088)\n",
      "\n",
      "Testing real-world knowledge with papuGaPT2 model:\n",
      "Prediction: Stolic Polski jest czyca\n",
      "Prediction: Najwy偶sz g贸r Polski s owiane\n",
      "Prediction: Najlepszy w historii pikarz z Argentyny to ..........\n",
      "Prediction: Stolic Niemiec jest r贸d Han\n",
      "Prediction: Stolic Francji jest 茅le\n",
      "Prediction: 2 wojn wiatow rozpoczli owcy i\n",
      "Prediction: 2 wojn wiatow wygrali n w\n",
      "\n",
      "Testing real-world knowledge with Bielik model:\n",
      "Prediction: Stolic Polski jest Warszawa. (Score: 0.24870306253433228)\n",
      "Prediction: Stolic Polski jest Gdask. (Score: 0.06255218386650085)\n",
      "Prediction: Stolic Polski jest Polska. (Score: 0.046921856701374054)\n",
      "Prediction: Najwy偶sz g贸r Polski s Niemcy. (Score: 0.27051886916160583)\n",
      "Prediction: Najwy偶sz g贸r Polski s Czechy. (Score: 0.1246902197599411)\n",
      "Prediction: Najwy偶sz g贸r Polski s Wgry. (Score: 0.10346551984548569)\n",
      "Prediction: Najlepszy w historii pikarz z Argentyny to Manuel. (Score: 0.02935340628027916)\n",
      "Prediction: Najlepszy w historii pikarz z Argentyny to Urugwaj. (Score: 0.019614826887845993)\n",
      "Prediction: Najlepszy w historii pikarz z Argentyny to Lorenzo. (Score: 0.0193090308457613)\n",
      "Prediction: Stolic Niemiec jest Polska. (Score: 0.08196217566728592)\n",
      "Prediction: Stolic Niemiec jest Warszawa. (Score: 0.06454018503427505)\n",
      "Prediction: Stolic Niemiec jest Berlin. (Score: 0.05627024546265602)\n",
      "Prediction: Stolic Francji jest Pary偶. (Score: 0.35811126232147217)\n",
      "Prediction: Stolic Francji jest Francja. (Score: 0.09537323564291)\n",
      "Prediction: Stolic Francji jest Belgia. (Score: 0.05163960158824921)\n",
      "Prediction: 2 wojn wiatow rozpoczli Niemcy. (Score: 0.3324713408946991)\n",
      "Prediction: 2 wojn wiatow rozpoczli Rosjanie. (Score: 0.16044853627681732)\n",
      "Prediction: 2 wojn wiatow rozpoczli Polacy. (Score: 0.12566083669662476)\n",
      "Prediction: 2 wojn wiatow wygrali Niemcy. (Score: 0.23802441358566284)\n",
      "Prediction: 2 wojn wiatow wygrali Rosjanie. (Score: 0.22466044127941132)\n",
      "Prediction: 2 wojn wiatow wygrali Polacy. (Score: 0.10382978618144989)\n"
     ]
    }
   ],
   "source": [
    "knowledge_sentences = [\n",
    "    \"Stolic Polski jest {mask}\",\n",
    "    \"Najwy偶sz g贸r Polski s {mask}\",\n",
    "    \"Najlepszy w historii pikarz z Argentyny to {mask}\",\n",
    "    \"Stolic Niemiec jest {mask}\",\n",
    "    \"Stolic Francji jest {mask}\",\n",
    "    \"2 wojn wiatow rozpoczli {mask}\",\n",
    "    \"2 wojn wiatow wygrali {mask}\"\n",
    "]\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    if model_info[\"pipeline\"]:\n",
    "        mask = masks.get(model_name, \"<mask>\")\n",
    "        print(f\"\\nTesting real-world knowledge with {model_name} model:\")\n",
    "        for sentence in knowledge_sentences:\n",
    "            sentence_with_mask = sentence.format(mask=mask)\n",
    "            outputs = model_info[\"pipeline\"](sentence_with_mask)\n",
    "            if model_name in [\"HerBERT\", \"Bielik\"]:\n",
    "                for output in outputs[:3]:  # Display top 3 predictions\n",
    "                    print(f\"Prediction: {output['sequence']} (Score: {output['score']})\")\n",
    "            elif model_name == \"papuGaPT2\":\n",
    "                for output in outputs[:3]:  # Display top 3 predictions\n",
    "                    print(f\"Prediction: {output['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check zero-shot learning capabilites of the models.\n",
    "Provide at least 5 sentences with different sentiment for the following scheme: \"'Ten film to by kiler. Nie mogem si oderwa od ekranu.' Wypowied藕 ta ma jest zdecydowanie [MASK]\" Try different prompts, to see if they make any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing zero-shot sentiment analysis with HerBERT model:\n",
      "Prediction: ' Ten film by super . Nie mogem si oderwa od ekranu . ' Wypowied藕 ta jest zdecydowanie prawdziwa . (Score: 0.19030681252479553)\n",
      "Prediction: ' Ten film by super . Nie mogem si oderwa od ekranu . ' Wypowied藕 ta jest zdecydowanie suszna . (Score: 0.13067613542079926)\n",
      "Prediction: ' Ten film by super . Nie mogem si oderwa od ekranu . ' Wypowied藕 ta jest zdecydowanie najlepsza . (Score: 0.12920497357845306)\n",
      "Prediction: ' To by bardzo nudny film , ledwo dotrwaem do koca . ' Wypowied藕 ta jest zdecydowanie prawdziwa . (Score: 0.2557086944580078)\n",
      "Prediction: ' To by bardzo nudny film , ledwo dotrwaem do koca . ' Wypowied藕 ta jest zdecydowanie suszna . (Score: 0.1446705013513565)\n",
      "Prediction: ' To by bardzo nudny film , ledwo dotrwaem do koca . ' Wypowied藕 ta jest zdecydowanie za . (Score: 0.08947570621967316)\n",
      "Prediction: ' Jestem zadowolony z tego zakupu , jako jest wietna ! ' Wypowied藕 ta jest zdecydowanie pozytywna . (Score: 0.24957124888896942)\n",
      "Prediction: ' Jestem zadowolony z tego zakupu , jako jest wietna ! ' Wypowied藕 ta jest zdecydowanie suszna . (Score: 0.17930330336093903)\n",
      "Prediction: ' Jestem zadowolony z tego zakupu , jako jest wietna ! ' Wypowied藕 ta jest zdecydowanie prawdziwa . (Score: 0.13994872570037842)\n",
      "Prediction: ' Nie polecam tego produktu , zawiodem si . ' Wypowied藕 ta jest zdecydowanie prawdziwa . (Score: 0.2596680223941803)\n",
      "Prediction: ' Nie polecam tego produktu , zawiodem si . ' Wypowied藕 ta jest zdecydowanie suszna . (Score: 0.25680145621299744)\n",
      "Prediction: ' Nie polecam tego produktu , zawiodem si . ' Wypowied藕 ta jest zdecydowanie uzasadniona . (Score: 0.07874782383441925)\n",
      "Prediction: ' Ksi偶ka bya ciekawa , i bardzo mnie wcigna . ' Wypowied藕 ta jest zdecydowanie prawdziwa . (Score: 0.23178239166736603)\n",
      "Prediction: ' Ksi偶ka bya ciekawa , i bardzo mnie wcigna . ' Wypowied藕 ta jest zdecydowanie pozytywna . (Score: 0.1011609360575676)\n",
      "Prediction: ' Ksi偶ka bya ciekawa , i bardzo mnie wcigna . ' Wypowied藕 ta jest zdecydowanie suszna . (Score: 0.09467945992946625)\n",
      "\n",
      "Testing zero-shot sentiment analysis with papuGaPT2 model:\n",
      "Prediction: 'Ten film by super. Nie mogem si oderwa od ekranu.' Wypowied藕 ta jest zdecydowanie owiana\n",
      "Prediction: 'To by bardzo nudny film, ledwo dotrwaem do koca.' Wypowied藕 ta jest zdecydowanie owiana\n",
      "Prediction: 'Jestem zadowolony z tego zakupu, jako jest wietna!' Wypowied藕 ta jest zdecydowanie .............\n",
      "Prediction: 'Nie polecam tego produktu, zawiodem si.' Wypowied藕 ta jest zdecydowanie owiana\n",
      "Prediction: 'Ksi偶ka bya ciekawa, i bardzo mnie wcigna.' Wypowied藕 ta jest zdecydowanie owiana\n",
      "\n",
      "Testing zero-shot sentiment analysis with Bielik model:\n",
      "Prediction: ' Ten film by super. Nie mogem si oderwa od ekranu.'Wypowied藕 ta jest zdecydowanie lepsza. (Score: 0.1264953315258026)\n",
      "Prediction: ' Ten film by super. Nie mogem si oderwa od ekranu.'Wypowied藕 ta jest zdecydowanie najlepsza. (Score: 0.09742730855941772)\n",
      "Prediction: ' Ten film by super. Nie mogem si oderwa od ekranu.'Wypowied藕 ta jest zdecydowanie najgorsza. (Score: 0.08607684820890427)\n",
      "Prediction: ' To by bardzo nudny film, ledwo dotrwaem do koca.'Wypowied藕 ta jest zdecydowanie lepsza. (Score: 0.1888660341501236)\n",
      "Prediction: ' To by bardzo nudny film, ledwo dotrwaem do koca.'Wypowied藕 ta jest zdecydowanie najlepsza. (Score: 0.07572013884782791)\n",
      "Prediction: ' To by bardzo nudny film, ledwo dotrwaem do koca.'Wypowied藕 ta jest zdecydowanie inna. (Score: 0.0637279748916626)\n",
      "Prediction: ' Jestem zadowolony z tego zakupu, jako jest wietna!'Wypowied藕 ta jest zdecydowanie lepsza. (Score: 0.35129478573799133)\n",
      "Prediction: ' Jestem zadowolony z tego zakupu, jako jest wietna!'Wypowied藕 ta jest zdecydowanie inna. (Score: 0.07824987173080444)\n",
      "Prediction: ' Jestem zadowolony z tego zakupu, jako jest wietna!'Wypowied藕 ta jest zdecydowanie najlepsza. (Score: 0.051519155502319336)\n",
      "Prediction: ' Nie polecam tego produktu, zawiodem si.'Wypowied藕 ta jest zdecydowanie lepsza. (Score: 0.23802347481250763)\n",
      "Prediction: ' Nie polecam tego produktu, zawiodem si.'Wypowied藕 ta jest zdecydowanie inna. (Score: 0.10677909851074219)\n",
      "Prediction: ' Nie polecam tego produktu, zawiodem si.'Wypowied藕 ta jest zdecydowanie gorsza. (Score: 0.05601195991039276)\n",
      "Prediction: ' Ksi偶ka bya ciekawa, i bardzo mnie wcigna.'Wypowied藕 ta jest zdecydowanie lepsza. (Score: 0.17026804387569427)\n",
      "Prediction: ' Ksi偶ka bya ciekawa, i bardzo mnie wcigna.'Wypowied藕 ta jest zdecydowanie prawdziwa. (Score: 0.06813622266054153)\n",
      "Prediction: ' Ksi偶ka bya ciekawa, i bardzo mnie wcigna.'Wypowied藕 ta jest zdecydowanie najlepsza. (Score: 0.06650715321302414)\n"
     ]
    }
   ],
   "source": [
    "sentiment_sentences = [\n",
    "    \"'Ten film by super. Nie mogem si oderwa od ekranu.' Wypowied藕 ta jest zdecydowanie {mask}\",\n",
    "    \"'To by bardzo nudny film, ledwo dotrwaem do koca.' Wypowied藕 ta jest zdecydowanie {mask}\",\n",
    "    \"'Jestem zadowolony z tego zakupu, jako jest wietna!' Wypowied藕 ta jest zdecydowanie {mask}\",\n",
    "    \"'Nie polecam tego produktu, zawiodem si.' Wypowied藕 ta jest zdecydowanie {mask}\",\n",
    "    \"'Ksi偶ka bya ciekawa, i bardzo mnie wcigna.' Wypowied藕 ta jest zdecydowanie {mask}\",\n",
    "]\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    if model_info[\"pipeline\"]:\n",
    "        mask = masks.get(model_name, \"<mask>\")\n",
    "        print(f\"\\nTesting zero-shot sentiment analysis with {model_name} model:\")\n",
    "        for sentence in sentiment_sentences:\n",
    "            sentence_with_mask = sentence.format(mask=mask)\n",
    "            outputs = model_info[\"pipeline\"](sentence_with_mask)\n",
    "            if model_name in [\"HerBERT\", \"Bielik\"]:\n",
    "                for output in outputs[:3]:  # Display top 3 predictions\n",
    "                    print(f\"Prediction: {output['sequence']} (Score: {output['score']})\")\n",
    "            elif model_name == \"papuGaPT2\":\n",
    "                for output in outputs[:3]:  # Display top 3 predictions\n",
    "                    print(f\"Prediction: {output['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following questions (2 points):\n",
    "\n",
    "1. Which of the models produced the best results?\n",
    "\n",
    "The best results were produced by the masked language models, HerBERT and Bielik. Overall, Bieliks responses were closer to the expected answers.\n",
    "\n",
    "2. Was any of the models able to capture Polish grammar?\n",
    "\n",
    "Both masked language models handled Polish grammar well, providing grammatically correct responses in each case. However, the responses from the causal language model were often incorrect.\n",
    "\n",
    "3. Was any of the models able to capture long-distant relationships between the words?\n",
    "\n",
    "Both masked language models captured long-distance relationships between words. However it should be clear from the context that the word the model predicts should be used in that specific gender.\n",
    "\n",
    "4. Was any of the models able to capture world knowledge?\n",
    "\n",
    "Partially. Both models were able to predict some world knowledge, such as country capitals and certain historical facts. However, they struggled with popular knowledge, like identifying Argentinas most famous football player or specific geographic data.\n",
    "\n",
    "5. Was any of the models good at doing zero-shot classification?\n",
    "\n",
    "No, neither model was precise in zero-shot classification. The predictions in each case were similar, making them difficult to distinguish.\n",
    "\n",
    "6. What are the most striking errors made by the models?\n",
    "\n",
    "The most striking errors were made by the causal language model. Its responses did not fit the context of the sentences. Since the GPT-2 architecture was not designed for masked language modeling, it struggled with filling masked tokens accurately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
